# ðŸ“Š Evaluation Plan: Module 03

This document outlines methods for evaluating the prompt structure and LLM responses.

## Methods
- **Human Scoring**: Analyst scores on relevance, grounding, tone, and clarity
- **LLM-as-Judge**: Use GPT-4 to compare prompt outputs across formats
- **Grounding Score**: % of signals in output that match input content
- **Format Validation**: Check for JSON schema correctness and key coverage
