# 🧠 Context Engineering Lab

> _Structured thinking, modular inputs, explainable outputs._  
> _A personal lab for designing and evaluating context-aware workflows for LLM systems._

---

## 📌 What is this?

**Context Engineering Lab** is a structured collection of design experiments around:

- 🧱 Structured Input Design (e.g. multi-modal prompt templates)
- 🔍 Vector Search & Retrieval (e.g. semantic chunking, FAISS)
- 🧠 LLM Integration (e.g. reasoning tasks, RAG pipelines)
- 📊 Prompt Evaluation (human-in-the-loop + automated metrics)

Each module explores a different angle of making LLMs **more grounded, interpretable, and production-ready**, especially in **security, behavior modeling**, and **explainability-critical** settings.

---

## 🧪 Modules

## 🧩 Modules

| No. | Module | Description |
|-----|--------|-------------|
| 01 | [Context Graph](./modules/module-01-context-graph) | Build reasoning flow via structured context graph |
| 02 | [Retrieval-Enhanced Prompting](./modules/module-02-Retrieval-Enhanced-Prompting) | Boost response quality using vector-based retrieval |
| 03 | [Structured ATO Evaluation](./modules/module-03-structured-input-ATO-eval) | Evaluate structured input pipelines for account takeover detection |
| 04 | [Agent Routing via Structured Context](./modules/module-04-Agent-Routing-via-Structured-Context) | Select and orchestrate agents based on parsed semantic schema |

---

## 🎯 Goals

- Explore **prompt architecture** from a systems + design perspective
- Apply **consulting-style reasoning (CCE: Complete, Conclusive, Explainable)** to LLM workflows
- Build and test patterns that improve grounding, control, and downstream integration

---

## 🚫 Not included

This lab does **not** focus on:
- Fine-tuning LLMs
- Proprietary tooling or closed-source platforms
- General prompt tips — this is about structured, testable input design

---

## 🗂️ Usage & Navigation

Each module includes:
- ✅ Markdown design doc (in `/modules`)
- 📓 Optional notebooks (in `/notebooks`)
- 🧪 Prompt templates + sample outputs (in `/examples`)
- 📊 Evaluation plans or logging scripts (in `/eval`)

---

👉 Try it: [matcha_prompt_blueprint_demo.ipynb](./demos/matcha_prompt_blueprint_demo.ipynb)

